{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Encontrando Doadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo do Projeto\n",
    "\n",
    "Este projeto tem como intuito utilizar diversos algoritmos de aprendizado supervisionado para modelar com precisão a remuneração de indivíduos utilizando dados coletados no censo americano de 1994. O objetivo é construir um modelo que pode predizer com precisão se um indivíduo possui uma remuneração superior a $50,000 e, desta forma, direcionar os esforços de uma instituição de caridade para os indivíduos que possuem uma maior probabilidade de realizar uma doação.\n",
    "\n",
    "O conjunto de dados para este projeto se origina do [Repositório de Machine Learning UCI](https://archive.ics.uci.edu/ml/datasets/Census+Income) e foi cedido por Ron Kohavi e Barry Becker, após a sua publicação no artigo _\"Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid\"_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na base de dados poderemos encontrar as seguintes informações:\n",
    "* **age**: contínuo. \n",
    "* **workclass**: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked. \n",
    "* **education**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool. \n",
    "* **education-num**: contínuo. \n",
    "* **marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse. \n",
    "* **occupation**: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces. \n",
    "* **relationship**: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried. \n",
    "* **race**: Black, White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other. \n",
    "* **sex**: Female, Male. \n",
    "* **capital-gain**: contínuo. \n",
    "* **capital-loss**: contínuo. \n",
    "* **hours-per-week**: contínuo. \n",
    "* **native-country**: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Explorando os dados\n",
    "A última coluna deste cojunto de dados, `'income'`, será o rótulo do nosso alvo (se um indivíduo possui remuneração igual ou maior do que $50,000 anualmente). Todas as outras colunas são dados de cada indívduo na base de dados do censo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education_level</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49</td>\n",
       "      <td>Private</td>\n",
       "      <td>9th</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Married-spouse-absent</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31</td>\n",
       "      <td>Private</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>14084.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>42</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>5178.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass education_level  education-num  \\\n",
       "0   39          State-gov       Bachelors           13.0   \n",
       "1   50   Self-emp-not-inc       Bachelors           13.0   \n",
       "2   38            Private         HS-grad            9.0   \n",
       "3   53            Private            11th            7.0   \n",
       "4   28            Private       Bachelors           13.0   \n",
       "5   37            Private         Masters           14.0   \n",
       "6   49            Private             9th            5.0   \n",
       "7   52   Self-emp-not-inc         HS-grad            9.0   \n",
       "8   31            Private         Masters           14.0   \n",
       "9   42            Private       Bachelors           13.0   \n",
       "\n",
       "           marital-status          occupation    relationship    race  \\\n",
       "0           Never-married        Adm-clerical   Not-in-family   White   \n",
       "1      Married-civ-spouse     Exec-managerial         Husband   White   \n",
       "2                Divorced   Handlers-cleaners   Not-in-family   White   \n",
       "3      Married-civ-spouse   Handlers-cleaners         Husband   Black   \n",
       "4      Married-civ-spouse      Prof-specialty            Wife   Black   \n",
       "5      Married-civ-spouse     Exec-managerial            Wife   White   \n",
       "6   Married-spouse-absent       Other-service   Not-in-family   Black   \n",
       "7      Married-civ-spouse     Exec-managerial         Husband   White   \n",
       "8           Never-married      Prof-specialty   Not-in-family   White   \n",
       "9      Married-civ-spouse     Exec-managerial         Husband   White   \n",
       "\n",
       "       sex  capital-gain  capital-loss  hours-per-week  native-country income  \n",
       "0     Male        2174.0           0.0            40.0   United-States  <=50K  \n",
       "1     Male           0.0           0.0            13.0   United-States  <=50K  \n",
       "2     Male           0.0           0.0            40.0   United-States  <=50K  \n",
       "3     Male           0.0           0.0            40.0   United-States  <=50K  \n",
       "4   Female           0.0           0.0            40.0            Cuba  <=50K  \n",
       "5   Female           0.0           0.0            40.0   United-States  <=50K  \n",
       "6   Female           0.0           0.0            16.0         Jamaica  <=50K  \n",
       "7     Male           0.0           0.0            45.0   United-States   >50K  \n",
       "8   Female       14084.0           0.0            50.0   United-States   >50K  \n",
       "9     Male        5178.0           0.0            40.0   United-States   >50K  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importando as bibliotecas necessárias para o projeto.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from IPython.display import display # Permite a utilização da função display() para DataFrames.\n",
    "import visuals as vs # Importação da biblioteca de visualização visuals.py\n",
    "import seaborn as sns # Biblioteca de visualização de dados estatísticos com base no matplotlib\n",
    "\n",
    "# Exibição amigável para notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Carregando os dados do Censo\n",
    "data = pd.read_csv(\"census.csv\")\n",
    "\n",
    "# Exibindo os 10 primeiros registros\n",
    "display(data.head(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui faremos uma investigação superficial da massa de dados que determinará quantos indivíduos se enquadram em cada grupo e nos dirá sobre o percentual destes indivúdos com remuneração anual superior à \\$50,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records: 45222\n",
      "Individuals making more than $50,000: 11208\n",
      "Individuals making at most $50,000: 34014\n",
      "Percentage of individuals making more than $50,000: 24.78%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>45222.000000</td>\n",
       "      <td>45222.000000</td>\n",
       "      <td>45222.000000</td>\n",
       "      <td>45222.000000</td>\n",
       "      <td>45222.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.547941</td>\n",
       "      <td>10.118460</td>\n",
       "      <td>1101.430344</td>\n",
       "      <td>88.595418</td>\n",
       "      <td>40.938017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.217870</td>\n",
       "      <td>2.552881</td>\n",
       "      <td>7506.430084</td>\n",
       "      <td>404.956092</td>\n",
       "      <td>12.007508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age  education-num  capital-gain  capital-loss  hours-per-week\n",
       "count  45222.000000   45222.000000  45222.000000  45222.000000    45222.000000\n",
       "mean      38.547941      10.118460   1101.430344     88.595418       40.938017\n",
       "std       13.217870       2.552881   7506.430084    404.956092       12.007508\n",
       "min       17.000000       1.000000      0.000000      0.000000        1.000000\n",
       "25%       28.000000       9.000000      0.000000      0.000000       40.000000\n",
       "50%       37.000000      10.000000      0.000000      0.000000       40.000000\n",
       "75%       47.000000      13.000000      0.000000      0.000000       45.000000\n",
       "max       90.000000      16.000000  99999.000000   4356.000000       99.000000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Número total de registros.\n",
    "n_records = len(data)\n",
    "\n",
    "# Número de registros com remuneração anual superior à $50,000\n",
    "n_greater_50k = 0\n",
    "\n",
    "# Número de registros com remuneração anual até $50,000\n",
    "n_at_most_50k = 0\n",
    "\n",
    "for income in data['income']:\n",
    "    if income == '>50K':\n",
    "        n_greater_50k += 1\n",
    "    else:\n",
    "        n_at_most_50k += 1\n",
    "\n",
    "# Percentual de indivíduos com remuneração anual superior à $50,000\n",
    "greater_percent = (n_greater_50k/n_records)*100\n",
    "\n",
    "# Exibindo os resultados\n",
    "print (\"Total number of records: {}\".format(n_records))\n",
    "print (\"Individuals making more than $50,000: {}\".format(n_greater_50k))\n",
    "print (\"Individuals making at most $50,000: {}\".format(n_at_most_50k))\n",
    "print (\"Percentage of individuals making more than $50,000: {:.2f}%\".format(greater_percent))\n",
    "\n",
    "# Gerando quadro com as estatísticas descritivas para as variáveis numéricas\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Selected KDE bandwidth is 0. Cannot estiamte density.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\nonparametric\\kde.py\u001b[0m in \u001b[0;36mkdensityfft\u001b[1;34m(X, kernel, bw, weights, gridsize, adjust, clip, cut, retgrid)\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[0mbw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'scott'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-c7bdfe70f58b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Gerando gráficos comparativos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpalette\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'husl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'income'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\axisgrid.py\u001b[0m in \u001b[0;36mpairplot\u001b[1;34m(data, hue, hue_order, palette, vars, x_vars, y_vars, kind, diag_kind, markers, height, aspect, corner, dropna, plot_kws, diag_kws, grid_kws, size)\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[0mdiag_kws\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shade\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2120\u001b[0m             \u001b[0mdiag_kws\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"legend\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2121\u001b[1;33m             \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_diag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkdeplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mdiag_kws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2123\u001b[0m     \u001b[1;31m# Maybe plot on the off-diagonals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\axisgrid.py\u001b[0m in \u001b[0;36mmap_diag\u001b[1;34m(self, func, **kwargs)\u001b[0m\n\u001b[0;32m   1488\u001b[0m                     \u001b[0mdata_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_na\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1490\u001b[1;33m                 \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1492\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clean_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py\u001b[0m in \u001b[0;36mkdeplot\u001b[1;34m(data, data2, shade, vertical, kernel, bw, gridsize, cut, clip, legend, cumulative, shade_lowest, cbar, cbar_ax, cbar_kws, ax, **kwargs)\u001b[0m\n\u001b[0;32m    703\u001b[0m         ax = _univariate_kdeplot(data, shade, vertical, kernel, bw,\n\u001b[0;32m    704\u001b[0m                                  \u001b[0mgridsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 705\u001b[1;33m                                  cumulative=cumulative, **kwargs)\n\u001b[0m\u001b[0;32m    706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py\u001b[0m in \u001b[0;36m_univariate_kdeplot\u001b[1;34m(data, shade, vertical, kernel, bw, gridsize, cut, clip, legend, ax, cumulative, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m         x, y = _statsmodels_univariate_kde(data, kernel, bw,\n\u001b[0;32m    294\u001b[0m                                            \u001b[0mgridsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                                            cumulative=cumulative)\n\u001b[0m\u001b[0;32m    296\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;31m# Fall back to scipy if missing statsmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py\u001b[0m in \u001b[0;36m_statsmodels_univariate_kde\u001b[1;34m(data, kernel, bw, gridsize, cut, clip, cumulative)\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[0mfft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkernel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"gau\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[0mkde\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKDEUnivariate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m     \u001b[0mkde\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgridsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgridsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcumulative\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkde\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkde\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\nonparametric\\kde.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, kernel, bw, fft, weights, gridsize, adjust, cut, clip)\u001b[0m\n\u001b[0;32m    138\u001b[0m             density, grid, bw = kdensityfft(endog, kernel=kernel, bw=bw,\n\u001b[0;32m    139\u001b[0m                     \u001b[0madjust\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madjust\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgridsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgridsize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                     clip=clip, cut=cut)\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             density, grid, bw = kdensity(endog, kernel=kernel, bw=bw,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\nonparametric\\kde.py\u001b[0m in \u001b[0;36mkdensityfft\u001b[1;34m(X, kernel, bw, weights, gridsize, adjust, clip, cut, retgrid)\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[0mbw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m         \u001b[0mbw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbandwidths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_bandwidth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkern\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# will cross-val fit this pattern?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m     \u001b[0mbw\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0madjust\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\nonparametric\\bandwidths.py\u001b[0m in \u001b[0;36mselect_bandwidth\u001b[1;34m(x, bw, kernel)\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;31m# eventually this can fall back on another selection criterion.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Selected KDE bandwidth is 0. Cannot estiamte density.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Selected KDE bandwidth is 0. Cannot estiamte density."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x900 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gerando gráficos comparativos\n",
    "sns.pairplot(data, palette = 'husl', hue='income')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Preparando os dados\n",
    "Antes de utilizarmos os dados como input para algoritmos de Machine Learning é necessário que eles sejam tratados, formatados e reestruturados (etapa de **pré-processamento**). Esta etapa irá otimizar o resultado e poder de predição de quase todos os algoritmos de aprendizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformando os principais desvios das colunas contínuas\n",
    "Um conjunto de dados pode conter variáveis onde os valores tendem a se próximar para um único número, mas também pode conter registros onde o mesmo atributo possuí um valor muito maior ou muito menor do que esta tendência. Algorítmos podem ser sensíveis para estes casos de distribuição de valores e este fator pode prejudicar sua performance se a distribuição não estiver normalizada de maneira adequada. Com o conjunto de dados do censo, dois atributos se encaixam nesta descrição: '`capital-gain'` e `'capital-loss'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo os dados entre features e coluna alvo\n",
    "income_raw = data['income']\n",
    "features_raw = data.drop('income', axis = 1)\n",
    "\n",
    "# Visualizando os principais desvios das colunas contínuas entre os dados\n",
    "vs.distribution(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para atributos com distribuição muito distorcida iremos aplicar uma <a href=\"https://en.wikipedia.org/wiki/Data_transformation_(statistics)\">transformação logarítmica</a> nos dados para que os valores muito grandes e muito pequenos não afetem a performance do algoritmo de aprendizado. Usar a transformação logarítmica reduz significativamente os limites dos valores afetados pelos outliers (valores muito grandes ou muito pequenos). Deve-se tomar cuidado ao aplicar esta transformação, poir o logaritmo de `0` é indefinido, portanto temos que incrementar os valores em uma pequena quantia acima de `0` para aplicar o logaritmo adequadamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando a transformação de log nos registros distorcidos.\n",
    "skewed = ['capital-gain', 'capital-loss']\n",
    "features_log_transformed = pd.DataFrame(data = features_raw)\n",
    "features_log_transformed[skewed] = features_raw[skewed].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "# Visualizando as novas distribuições após a transformação.\n",
    "vs.distribution(features_log_transformed, transformed = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizando atributos numéricos\n",
    "Como boa prática vamos realizar algum tipo de adaptação de escala nos atributos numéricos. Ajustar a escala nos dados não modifica o formato da distribuição de cada coluna (tais como `'capital-gain'` ou `'capital-loss'` acima); no entanto, a normalização garante que cada atributo será tratado com o mesmo peso durante a aplicação de aprendizado supervisionado. Uma vez aplicada a escala, a observação dos dados não terá o significado original, como pode-se ver abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando sklearn.preprocessing.StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Inicializando um aplicador de escala e aplicando em seguida aos atributos\n",
    "scaler = MinMaxScaler() # default=(0, 1)\n",
    "numerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "features_log_minmax_transform = pd.DataFrame(data = features_log_transformed)\n",
    "features_log_minmax_transform[numerical] = scaler.fit_transform(features_log_transformed[numerical])\n",
    "\n",
    "# Exibindo um exemplo de registro com a escala aplicada\n",
    "display(features_log_minmax_transform.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que em nosso conjunto de dados existem diversos atributos não-numéricos para cada registro. Usualmente, algoritmos de aprendizado esperam que os inputs sejam numéricos, o que requer que os atributos não numéricos (*variáveis categóricas*) sejam convertidos. Utilizaremos a estratégia **one-hot encoding**. Esta estratégia cria uma variável para cada categoria possível de cada atributo não numérico. Por exemplo, assuma que `algumAtributo` possuí três valores possíveis: `A`, `B`, ou `C`. Nós então transformamos este atributo em três novos atributos: `algumAtributo_A`, `algumAtributo_B` e `algumAtributo_C`.\n",
    "\n",
    "\n",
    "|   | algumAtributo |                    | algumAtributo_A | algumAtributo_B | algumAtributo_C |\n",
    "| :-: | :-: |                            | :-: | :-: | :-: |\n",
    "| 0 |  B  |  | 0 | 1 | 0 |\n",
    "| 1 |  C  | ----> one-hot encode ----> | 0 | 0 | 1 |\n",
    "| 2 |  A  |  | 1 | 0 | 0 |\n",
    "\n",
    "Além disso, assim como os atributos não-numéricos, precisaremos converter a coluna alvo não-numérica, `'income'`, para valores numéricos para que o algoritmo de aprendizado funcione. Uma vez que só existem duas categorias possíveis para esta coluna (\"<=50K\" e \">50K\"), nós podemos evitar a utilização do one-hot encoding e simplesmente transformar estas duas categorias para `0` e `1`, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Realizando o one-hot encoding nos dados em 'features_log_minmax_transform' utilizando pandas.get_dummies()\n",
    "features_final = pd.get_dummies(features_log_minmax_transform)\n",
    "\n",
    "# Realizando o encode da coluna 'income_raw' para valores numéricos\n",
    "income = income_raw.apply(lambda x: 1 if x == \">50K\" else 0)\n",
    "\n",
    "# Exibe o número de colunas depois do one-hot encoding\n",
    "encoded = list(features_final.columns)\n",
    "print (\"{} total features after one-hot encoding.\".format(len(encoded)))\n",
    "\n",
    "# Descomente a linha abaixo para ver as colunas após o encode\n",
    "#print (encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embaralhar e dividir os dados\n",
    "Agora todas as _variáveis de categoria_ foram convertidas em atributos numéricos e todos os atributos numéricos foram normalizados. Então, agora dividiremos os dados entre conjuntos de treinamento e de teste. 80% dos dados serão utilizados para treinamento e 20% para teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir os 'atributos' e 'income' entre conjuntos de treinamento e de testes.\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_final, \n",
    "                                                    income, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 0)\n",
    "\n",
    "# Mostrar os resultados da divisão\n",
    "print (\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Avaliando a performance do modelo\n",
    "Nesta seção nós investigaremos quatro algoritmos diferentes e determinaremos qual deles é melhor para a modelagem dos dados: Naive Predictor, Florestas Aleatórias, Bagging e SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas e o Naive predictor\n",
    "\n",
    "Sabemos que os indivíduos que fazem mais do que \\$50,000 possuem uma maior probabilidade de doar para uma campanha de caridade. Por conta disto, iremos predizer com acurácia quais indivíduos possuem remuneração acima de \\$50,000. Utilizar **acurácia (accuracy)** como uma métrica para avaliar a performance de um modelo é o parâmetro adequado porque identificar alguém que *não possui* remuneração acima de \\$50,000 como alguém que recebe acima deste valor seria ruim para a *CharityML*, uma vez que eles estão procurando por indivíduos que desejam doar. Com isso, a habilidade do modelo em predizer com preisão aqueles que possuem a remuneração acima dos \\$50,000 é *mais importante* do que a habilidade de realizar o **recall** destes indivíduos. Nós podemos utilizar a fórmula **F-beta score** como uma métrica que considera ambos: precision e recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas e o Naive predictor\n",
    "\n",
    "Sabemos que os indivíduos que fazem mais do que 50,000 possuem maior probabilidade de doar para esta campanha de caridade. Por conta disto, estamos interessados em predizer com acurácia quais indivíduos possuem remuneração acima de 50,000.\n",
    "\n",
    "Utilizaremos a **acurácia (accuracy)** como uma métrica para avaliar a performance do modelo porque não queremos direcionar os esforços da campanha para indivíduos que possuem uma remuneração abaixo deste valor. Com isso, a habilidade do modelo em predizer com precisão aqueles que possuem a remuneração acima dos \\$50,000 é *mais importante* do que a habilidade de realizar o **recall** destes indivíduos. Iremos utilizar a fórmula **F-beta score** como uma métrica que considera ambos: precision e recall.\n",
    "\n",
    "\n",
    "$$ F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{precision \\cdot recall}{\\left( \\beta^2 \\cdot precision \\right) + recall} $$\n",
    "\n",
    "Em particular, quando $\\beta = 0.5$, maior ênfase é atribuída para a variável precision. Isso é chamado de **F$_{0.5}$ score** (ou F-score, simplificando).\n",
    "\n",
    "Analisando a distribuição de classes (aqueles que possuem remuneração até 50,000 e aqueles que possuem remuneração superior), fica claro que a maioria dos indivíduos não possui remuneração acima de 50,000. Isto pode ter grande impacto na **acurácia (accuracy)**, uma vez que nós poderíamos simplesmente dizer *\"Esta pessoa não possui remuneração acima de \\$50,000\"* e estar certos em boa parte das vezes, sem ao menos olhar os dados! Fazer este tipo de afirmação seria chamado de **naive**, uma vez que não consideramos nenhuma informação para balisar este argumento. É sempre importante considerar a *naive prediction* para seu conjunto de dados, para ajudar a estabelecer um benchmark para análise da performance dos modelos. Com isso, sabemos que utilizar a naive prediction não traria resultado algum: Se a predição apontasse que todas as pessoas possuem remuneração inferior à \\$50,000, a *CharityML* não identificaria ninguém como potencial doador. \n",
    "\n",
    "\n",
    "\n",
    "#### Nota: Revisando: accuracy, precision e recall\n",
    "\n",
    "Considere um problema onde você deve classificar se um email recebido deve ir ou não para a caixa de spam:\n",
    "\n",
    "** Accuracy ** mede com que frequência o classificador faz a predição correta. É a proporção entre o número de predições corretas e o número total de predições (o número de registros testados).\n",
    "\n",
    "** Precision ** informa qual a proporção de mensagens classificamos como spam eram realmente spam. Ou seja, é a proporção de verdadeiros positivos (mensagens classificadas como spam que eram realmente spam) sobre todos os positivos (todas as palavras classificadas como spam, independente se a classificação estava correta), em outras palavras, é a proporção\n",
    "\n",
    "`[Verdadeiros positivos/(Verdadeiros positivos + Falso positivos)]`\n",
    "\n",
    "** Recall(sensibilidade)** nos informa qual a proporção das mensagens que eram spam que foram corretamente classificadas como spam. É a proporção entre os verdadeiros positivos (classificados como spam, que realmente eram spam) sobre todas as palavras que realmente eram spam. Em outras palavras, é a proporção entre\n",
    "\n",
    "`[Verdadeiros positivos/(Verdadeiros positivos + Falso negativos)]`\n",
    "\n",
    "Para problemas de classificação distorcidos em suas distribuições, como no nosso caso, por exemplo, se tivéssemos 100 mensagems de texto e apenas 2 fossem spam e todas as outras não fossem, a \"accuracy\" por si só não seria uma métrica tão boa. Nós poderiamos classificar 90 mensagems como \"não-spam\" (incluindo as 2 que eram spam mas que teriam sido classificadas como não-spam e, por tanto, seriam falso negativas.) e 10 mensagems como spam (todas as 10 falso positivas) e ainda assim teriamos uma boa pontuação de accuracy. Para estess casos, precision e recall são muito úteis. Estas duas métricas podem ser combinadas para resgatar o F1 score, que é calculado através da média(harmônica) dos valores de precision e de recall. Este score pode variar entre 0 e 1, sendo 1 o melhor resultado possível para o F1 score (consideramos a média harmônica pois estamos lidando com proporções)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance do Naive Predictor\n",
    "* Agora iremos criar um modelo que sempre prediz que um indivíduo possui remuneração acima de $50,000.\n",
    "\n",
    "** O propósito ao gerar um naive predictor é simplesmente exibir como um modelo sem nenhuma inteligência se comportaria. No mundo real, idealmente o seu modelo de base será o resultado de um modelo anterior ou poderia ser baseado em um paper no qual você se basearia para melhorar. Quando não houver qualquer benchmark de modelo, utilizar um naive predictor será melhor do que uma escolha aleatória.\n",
    "\n",
    "* Quando temos um modelo que sempre prediz '1' (e.x o indivíduo possui remuneração superior à 50k) então nosso modelo não terá Verdadeiros Negativos ou Falso Negativos, pois nós não estaremos afirmando que qualquer dos valores é negativo (ou '0') durante a predição. Com isso, nossa accuracy neste caso se torna o mesmo valor da precision (Verdadeiros positivos/ (Verdadeiros positivos + Falso positivos)) pois cada predição que fizemos com o valor '1' que deveria ter o valor '0' se torna um falso positivo; nosso denominador neste caso é o número total de registros.\n",
    "* Nossa pontuação de Recall(Verdadeiros positivos/(Verdadeiros Positivos + Falsos negativos)) será 1 pois não teremos Falsos negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TP = Este é o caso \"naive\". Note que 'income' são os dados 'income_raw' convertidos\n",
    "para valores numéricos durante o passo de pré-processamento de dados.\n",
    "FP = Específico para o caso naive\n",
    "TN = Sem predições negativas para o caso naive\n",
    "FN = Sem predições negativas para o caso naive\n",
    "'''\n",
    "# Calcular accuracy, precision e recall\n",
    "TP = float(np.sum(income))\n",
    "FP = float(income.count() - TP)\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "accuracy = float((TP + FN)/ (TP + FP + TN + FN))\n",
    "recall = float(TP / (TP + FN))\n",
    "precision = float(TP/(TP+FP))\n",
    "b = 0.5\n",
    "#Calcular o F-score utilizando a fórmula acima para o beta = 0.5 e os valores corretos de precision e recall.\n",
    "fscore = (1+b**2) * (precision * recall) / (b**2*precision + recall)\n",
    "\n",
    "# Exibir os resultados \n",
    "print (\"Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\".format(accuracy, fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Modelos de Aprendizado Supervisionado\n",
    "**Estes são alguns dos modelos de aprendizado supervisionado disponíveis em** [`scikit-learn`](http://scikit-learn.org/stable/supervised_learning.html)\n",
    "- Gaussian Naive Bayes (GaussianNB)\n",
    "- Decision Trees (Árvores de decisão)\n",
    "- Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting)\n",
    "- K-Nearest Neighbors (KNeighbors)\n",
    "- Stochastic Gradient Descent Classifier (SGDC)\n",
    "- Support Vector Machines (SVM)\n",
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação do Modelo\n",
    "Liste três dos modelos de aprendizado supervisionado acima que são apropriados para este problema que você irá testar nos dados do censo. Para cada modelo escolhido\n",
    "\n",
    "- Descreva uma situação do mundo real onde este modelo pode ser utilizado. \n",
    "- Quais são as vantagems da utilização deste modelo; quando ele performa bem?\n",
    "- Quais são as fraquez\n",
    "as do modelo; quando ele performa mal?\n",
    "- O que torna este modelo um bom candidato para o problema, considerando o que você sabe sobre o conjunto de dados?\n",
    "\n",
    "** DICA: **\n",
    "\n",
    "Estruture sua resposta no mesmo formato acima^, com 4 partes para cada um dos modelos que você escolher. Por favor, inclua referências em cada uma das respostas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos treinar os dados com base nestes 3 modelos de aprendizagem supervisionada:\n",
    "\n",
    "**Florestas Aleatórias**\n",
    "1. Este método pode ser utilizado em problemas onde há a necessidade de segmentação do cluster, por exemplo, na criação de perfis no Spotify que guiem a sugestão da música mais apropriada para um indivíduo;\n",
    "2. Por meio de perguntas binárias, este modelo vai criando uma árvore de decisão que conforme recebe as respostas conduz para um determinado resultado. Diferentemente da Árvore de Decisão, ele combina diversas árvores com o intuito de saber quais Features resumem melhor os dados. Por isto, é recomendado para bases que possuímos muitas categorias.\n",
    "3. Como ponto negativo temos o fato que quando a árvore começa ramificar muito (aumentar sua profundidade), há um risco de sobreposição, não generalizando bem os dados.\n",
    "4. Podemos utilizar este método para o nosso problema porque ele permite analisar as Features que temos e definir quais indivíduos devem receber a carta de pedido de doação. Além disto, a quantidade de dados e o fato da maioria das Features serem categóricas, reforçam a escolha do modelo\n",
    "\n",
    "**Bagging**\n",
    "1. Neste método, vários 'strong learners' (estimadores próximos ao overfitting) são agrupadas com o intuito de criar um preditor melhor. No mundo real pode ser utilizado junto com SVM, Regressão ou outro método que exija um modelo mais estável, por exemplo, na concessão de crédito ao consumidor.\n",
    "2. Atua combinando estimadores de alta variância e, desta forma, diminui a variância total através da média ou do método major vote.\n",
    "3. Em procedimentos estáveis, o preditor bagging não traz melhora e pode até ter desempenho pior. \n",
    "4. Esta técnica pode ser utilizada para minimizar as chances de construirmos um modelo overfitting e, consequentemente, economizar no envio de correspondências para pessoas fora do perfil. \n",
    "\n",
    "**SVM**\n",
    "1. É uma ferramente de classificação e análise de regressão dos dados que também auxilia no separação do melhor limite entre os hiperplanos considerando a minimização do erro (erro de classificação + magem de erro)\n",
    "2. Funciona muito bem em domínios complicados, em que existe uma clara margem de separação. É eficaz em espaços dimensionais elevados e bastante versátilpor conta das diferentes funções de Kernel que podem ser espicificadas;\n",
    "3. Grandes conjuntos de dados irão exigir uma alta complexidade operacional e também não funciona bem em conjuntos com muitos ruídos.\n",
    "4. Como temos um problema de classificação e uma base não tão grande, o SVM poderá nos ajudar a separar os potenciais doadores da população restante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando um Pipeline de Treinamento e Predição\n",
    "Para avaliar adequadamente a performance de cada um dos modelos que você escolheu é importante que você crie um pipeline de treinamento e predição que te permite de maneira rápida e eficiente treinar os modelos utilizando vários tamanhos de conjuntos de dados para treinamento, além de performar predições nos dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando fbeta_score e accuracy_score\n",
    "\n",
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Adapta o algoritmo para os dados de treinamento e registra o tempo de treinamento.\n",
    "    start = time() # Get start time\n",
    "    learner = learner.fit(X_train[:sample_size], y_train[:sample_size])\n",
    "    end = time() # Get end time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    # Realiza as predições nos dados de teste X_test, e também nos 300 primeiros pontos de treinamento X_train[:300]\n",
    "    start = time() # Start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train[:300])\n",
    "    end = time() # End time\n",
    "    \n",
    "    # Calcula o tempo total de predição\n",
    "    results['pred_time'] = end - start\n",
    "            \n",
    "    # Computa a Acuracia das 300  primeiras amostras de treino - y_train[:300]\n",
    "    results['acc_train'] = accuracy_score(y_train[:300],predictions_train)\n",
    "        \n",
    "    # Computa a acuracia do conjunto de teste utilizando accuracy_score()\n",
    "    results['acc_test'] = accuracy_score(y_test, predictions_test)\n",
    "    \n",
    "    # Computa o F-score das 300  primeiras amostras de treino - fbeta_score()\n",
    "    results['f_train'] = fbeta_score(y_train[:300] , predictions_train, beta = 0.5)\n",
    "        \n",
    "    # Computa F-score ono conjunto de teste que é y_test\n",
    "    results['f_test'] = fbeta_score(y_test , predictions_test, beta = 0.5)\n",
    "       \n",
    "    # Mostra resultados\n",
    "    print (\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
    "        \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação inicial do modelo\n",
    "Agora iremos realizar as seguintes etapas:\n",
    "- Importar os três modelos de aprendizado supervisionado  \n",
    "- Inicializar os três modelos e armazená-los em `'clf_A'`, `'clf_B'`, e `'clf_C'`. \n",
    "- Calcular o número de registros equivalentes à 1%, 10%, e 100% dos dados de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando os três modelos de aprendizado supervisionado da sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Inicializando os três modelos\n",
    "clf_A = RandomForestClassifier(random_state = 10)\n",
    "clf_B = BaggingClassifier (random_state = 10)\n",
    "clf_C = SVC (random_state = 10)\n",
    "\n",
    "# Calculando o número de amostras para 1%, 10%, e 100% dos dados de treinamento\n",
    "samples_100 = len(y_train)\n",
    "samples_10 = int(len(y_train)*10/100)\n",
    "samples_1 = int(len(y_train)*1/100)\n",
    "\n",
    "# Coletando os resultados dos algoritmos de aprendizado\n",
    "results = {}\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n",
    "        results[clf_name][i] = \\\n",
    "        train_predict(clf, samples, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Gerando gráficos de visualização para os 3 modelos\n",
    "vs.evaluate(results, accuracy, fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Melhorando os resultados\n",
    "Nesta seção iremos escolher o melhor entre os três modelos de aprendizado supervisionado.\n",
    "\n",
    "### Escolhendo o melhor modelo\n",
    "\n",
    "Antes de escolher o modelo vamos entender o significado do score.\n",
    "Quando um modelo possuí um score alto, quer dizer que ele possuí uma alta variância (overfitting), ou seja, ele se ajusta muito bem aos dados de teste, pondendo capturar inclusive os ruídos como se estivesse decorando os dados, pode ter problemas para se adaptar à novos dados.\n",
    "\n",
    "Já modelos com um score baixo (underfitting) são muito genéricos e falham em captar a variância dos dados.\n",
    "A primeira informação que temos é que o método SVC demanda uma capacidade operacional muito superior às outras ferramentas, junto a isto ainda há o fato de ter a Accuracy e o F-score baixos para os dados de treino, sendo então o primeiro a ser descartado.\n",
    "\n",
    "Entre o método Bagging e Florestas Aleatórias os resultados são muito parecidos, chegando em um Accuracy próximo de 100% nos dados de treino e ~80% nos dados de teste, enquanto o tempo de processamento fica abaixo de 1 segundo. Por isto, para escolher o melhor modelo será utilizado como fator de desempate a adequação do algoritmo para este conjunto.\n",
    "Portanto, o modelo escolhido é Florestas Aleatórios que suporta a análise em conjunto de diversas features categóricas, além de se adaptar ao tamanho da base que temos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entendendo mais sobre o modelo\n",
    "\n",
    "O método Florestas Aleatórias é muito utilizado quando queremos segmentar e classificar os usuários em perfis, no nosso caso: Potenciais Doadores e Não Doadores. \n",
    "\n",
    "Por meio de análises estatísticas, todas as características disponíveis na base como sexo, idade, nível de educação, ocupação, etc. são analisadas em conjunto pelo próprio modelo por meio de perguntas que possuem resposta binárias (sim/não). Por exemplo, para analisar a característica Idade o sistema poderá fazer a pergunta \"Este cliente tem uma idade maior que 40 anos?\" e, se ao analisar a base toda houver uma confirmação que há um padrão e esta é uma característica relevante para classificar um doador, então ela será considerada para as futuras predições, podendo também ser combinadas com outras como: maior que 40 anos e do sexo feminino. \n",
    "\n",
    "No nosso caso, como temos diversas características que podem criar inúmeros perfis diferentes de doadores, nós criaremos várias árvores de decisão diferentes e, combinando elas, temos quais são os perfis que melhor se enquadram os potenciais doadores da CharityML.\n",
    "\n",
    "Abaixo um exemplo de uma Floresta Aleatória:\n",
    "\n",
    "![title](img/florest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinando o modelo\n",
    "Refine o modelo escolhido. Utilize uma busca grid (`GridSearchCV`) com pelo menos um parâmetro importante refinado com pelo menos 3 valores diferentes. Você precisará utilizar todo o conjunto de treinamento para isso. Na célula de código abaixo, você precisará implementar o seguinte:\n",
    "- Importar [`sklearn.grid_search.GridSearchCV`](http://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.GridSearchCV.html) e [`sklearn.metrics.make_scorer`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html).\n",
    "- Inicializar o classificador escolhido por você e armazená-lo em `clf`.\n",
    " - Configurar um `random_state` se houver um disponível para o mesmo estado que você configurou anteriormente.\n",
    "- Criar um dicionário dos parâmetros que você quer otimizar para o modelo escolhido.\n",
    " - Exemplo: `parâmetro = {'parâmetro' : [lista de valores]}`.\n",
    " - **Nota:** Evite otimizar o parâmetro `max_features` se este parâmetro estiver disponível! \n",
    "- Utilize `make_scorer` para criar um objeto de pontuação `fbeta_score` (com $\\beta = 0.5$).\n",
    "- Realize a busca gride no classificador `clf` utilizando o `'scorer'` e armazene-o na variável `grid_obj`.   \n",
    "- Adeque o objeto da busca grid aos dados de treino (`X_train`, `y_train`) e armazene em `grid_fit`.\n",
    "\n",
    "**Nota:** Dependendo do algoritmo escolhido e da lista de parâmetros, a implementação a seguir pode levar algum tempo para executar! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=2, n_estimators=13, score=0.674, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=2, n_estimators=13, score=0.674, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.5s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=2, n_estimators=13, score=0.666, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.4s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=2, n_estimators=13, score=0.682, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    3.2s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=2, n_estimators=13, score=0.688, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    4.0s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=4, n_estimators=13, score=0.685, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    4.7s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=4, n_estimators=13, score=0.689, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    5.6s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=4, n_estimators=13, score=0.687, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    6.3s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=4, n_estimators=13, score=0.689, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    7.1s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=4, n_estimators=13, score=0.701, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    7.9s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=6, n_estimators=13, score=0.692, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    8.7s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=6, n_estimators=13, score=0.703, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    9.5s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=6, n_estimators=13, score=0.702, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:   10.2s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=6, n_estimators=13, score=0.700, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:   11.0s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=6, n_estimators=13, score=0.708, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   11.7s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=8, n_estimators=13, score=0.706, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:   12.4s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=8, n_estimators=13, score=0.707, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:   13.1s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=8, n_estimators=13, score=0.706, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:   13.8s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=8, n_estimators=13, score=0.707, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:   14.6s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=1, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=1, min_samples_split=8, n_estimators=13, score=0.705, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   15.4s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=2, n_estimators=13, score=0.732, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:   16.0s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=2, n_estimators=13, score=0.731, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:   16.7s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=2, n_estimators=13, score=0.725, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:   17.3s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=2, n_estimators=13, score=0.723, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:   17.9s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=2, n_estimators=13, score=0.730, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:   18.5s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=4, n_estimators=13, score=0.732, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:   19.2s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=4, n_estimators=13, score=0.731, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:   19.8s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=4, n_estimators=13, score=0.725, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:   20.4s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=4, n_estimators=13, score=0.723, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:   21.0s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=4, n_estimators=13, score=0.730, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:   21.7s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=6, n_estimators=13 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=6, n_estimators=13, score=0.726, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:   22.3s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=6, n_estimators=13, score=0.734, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:   22.9s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=6, n_estimators=13, score=0.727, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:   23.5s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=6, n_estimators=13, score=0.729, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed:   24.1s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=6, n_estimators=13, score=0.733, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed:   24.8s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=8, n_estimators=13, score=0.734, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:   25.4s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=8, n_estimators=13, score=0.731, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  37 out of  37 | elapsed:   26.0s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=8, n_estimators=13, score=0.727, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed:   26.6s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=8, n_estimators=13, score=0.725, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed:   27.2s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=2, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=2, min_samples_split=8, n_estimators=13, score=0.731, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:   27.8s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=2, n_estimators=13, score=0.729, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  41 out of  41 | elapsed:   28.4s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=2, n_estimators=13, score=0.735, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:   29.0s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=2, n_estimators=13, score=0.729, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  43 out of  43 | elapsed:   29.5s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=2, n_estimators=13, score=0.724, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  44 out of  44 | elapsed:   30.1s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=2, n_estimators=13, score=0.735, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:   30.7s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=4, n_estimators=13, score=0.729, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  46 out of  46 | elapsed:   31.3s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=4, n_estimators=13, score=0.735, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  47 out of  47 | elapsed:   31.9s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=4, n_estimators=13, score=0.729, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:   32.5s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=4, n_estimators=13, score=0.724, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  49 out of  49 | elapsed:   33.0s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=4, n_estimators=13, score=0.735, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:   33.6s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=6, n_estimators=13, score=0.729, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  51 out of  51 | elapsed:   34.2s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=6, n_estimators=13, score=0.735, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  52 out of  52 | elapsed:   34.8s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=6, n_estimators=13, score=0.729, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  53 out of  53 | elapsed:   35.4s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=6, n_estimators=13, score=0.724, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  54 out of  54 | elapsed:   36.0s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=6, n_estimators=13, score=0.735, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  55 out of  55 | elapsed:   36.6s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=8, n_estimators=13, score=0.726, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  56 out of  56 | elapsed:   37.2s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=8, n_estimators=13, score=0.731, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed:   37.7s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=8, n_estimators=13, score=0.720, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  58 out of  58 | elapsed:   38.3s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=8, n_estimators=13, score=0.728, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  59 out of  59 | elapsed:   38.9s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=3, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=3, min_samples_split=8, n_estimators=13, score=0.732, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   39.5s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=2, n_estimators=13, score=0.730, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  61 out of  61 | elapsed:   40.0s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=2, n_estimators=13 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=2, n_estimators=13, score=0.739, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  62 out of  62 | elapsed:   40.6s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=2, n_estimators=13, score=0.727, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:   41.1s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=2, n_estimators=13, score=0.723, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:   41.7s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=2, n_estimators=13, score=0.729, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  65 out of  65 | elapsed:   42.3s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=4, n_estimators=13, score=0.730, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  66 out of  66 | elapsed:   42.8s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=4, n_estimators=13, score=0.739, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  67 out of  67 | elapsed:   43.4s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=4, n_estimators=13, score=0.727, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  68 out of  68 | elapsed:   43.9s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=4, n_estimators=13, score=0.723, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  69 out of  69 | elapsed:   44.6s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=4, n_estimators=13, score=0.729, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed:   45.2s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=6, n_estimators=13, score=0.730, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  71 out of  71 | elapsed:   45.8s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=6, n_estimators=13, score=0.739, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:   46.4s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=6, n_estimators=13, score=0.727, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  73 out of  73 | elapsed:   46.9s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=6, n_estimators=13, score=0.723, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  74 out of  74 | elapsed:   47.5s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=6, n_estimators=13, score=0.729, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:   48.1s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=8, n_estimators=13, score=0.730, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  76 out of  76 | elapsed:   48.6s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=8, n_estimators=13, score=0.739, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  77 out of  77 | elapsed:   49.2s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=8, n_estimators=13, score=0.727, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  78 out of  78 | elapsed:   49.7s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=8, n_estimators=13, score=0.723, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  79 out of  79 | elapsed:   50.3s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=4, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=4, min_samples_split=8, n_estimators=13, score=0.729, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:   50.9s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=2, n_estimators=13, score=0.725, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  81 out of  81 | elapsed:   51.4s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=2, n_estimators=13, score=0.736, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  82 out of  82 | elapsed:   52.0s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=2, n_estimators=13, score=0.730, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  83 out of  83 | elapsed:   52.5s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=2, n_estimators=13, score=0.720, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  84 out of  84 | elapsed:   53.1s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=2, n_estimators=13, score=0.732, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  85 out of  85 | elapsed:   53.6s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=4, n_estimators=13, score=0.725, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  86 out of  86 | elapsed:   54.2s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=4, n_estimators=13, score=0.736, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  87 out of  87 | elapsed:   54.7s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=4, n_estimators=13, score=0.730, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  88 out of  88 | elapsed:   55.3s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=4, n_estimators=13, score=0.720, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  89 out of  89 | elapsed:   55.8s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=4, n_estimators=13, score=0.732, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:   56.4s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=6, n_estimators=13, score=0.725, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  91 out of  91 | elapsed:   56.9s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=6, n_estimators=13, score=0.736, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  92 out of  92 | elapsed:   57.5s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=6, n_estimators=13 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=6, n_estimators=13, score=0.730, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  93 out of  93 | elapsed:   58.0s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=6, n_estimators=13, score=0.720, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  94 out of  94 | elapsed:   58.6s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=6, n_estimators=13, score=0.732, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:   59.2s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=8, n_estimators=13, score=0.725, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  96 out of  96 | elapsed:   59.7s remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=8, n_estimators=13, score=0.736, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  97 out of  97 | elapsed:  1.0min remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=8, n_estimators=13, score=0.730, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  98 out of  98 | elapsed:  1.0min remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=8, n_estimators=13, score=0.720, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  99 out of  99 | elapsed:  1.0min remaining:    0.0s\n",
      "[CV] criterion=gini, min_samples_leaf=5, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=5, min_samples_split=8, n_estimators=13, score=0.732, total=   0.6s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=2, n_estimators=13, score=0.721, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=2, n_estimators=13, score=0.735, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=2, n_estimators=13, score=0.728, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=2, n_estimators=13, score=0.729, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=2, n_estimators=13, score=0.727, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=4, n_estimators=13, score=0.721, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=4, n_estimators=13, score=0.735, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=4, n_estimators=13, score=0.728, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=4, n_estimators=13, score=0.729, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=4, n_estimators=13, score=0.727, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=6, n_estimators=13, score=0.721, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=6, n_estimators=13, score=0.735, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=6, n_estimators=13, score=0.728, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=6, n_estimators=13, score=0.729, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=6, n_estimators=13, score=0.727, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=8, n_estimators=13, score=0.721, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=8, n_estimators=13, score=0.735, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=8, n_estimators=13, score=0.728, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=8, n_estimators=13, score=0.729, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=6, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=6, min_samples_split=8, n_estimators=13, score=0.727, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=2, n_estimators=13, score=0.723, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=2, n_estimators=13, score=0.730, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=2, n_estimators=13, score=0.725, total=   0.6s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=2, n_estimators=13, score=0.722, total=   0.6s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=2, n_estimators=13, score=0.732, total=   0.6s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=4, n_estimators=13, score=0.723, total=   0.6s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=4, n_estimators=13, score=0.730, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=4, n_estimators=13, score=0.725, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=4, n_estimators=13, score=0.722, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=4, n_estimators=13, score=0.732, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=6, n_estimators=13, score=0.723, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=6, n_estimators=13, score=0.730, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=6, n_estimators=13, score=0.725, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=6, n_estimators=13, score=0.722, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=6, n_estimators=13 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=6, n_estimators=13, score=0.732, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=8, n_estimators=13, score=0.723, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=8, n_estimators=13, score=0.730, total=   0.6s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=8, n_estimators=13, score=0.725, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=8, n_estimators=13, score=0.722, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=7, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=7, min_samples_split=8, n_estimators=13, score=0.732, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=2, n_estimators=13, score=0.724, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=2, n_estimators=13, score=0.728, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=2, n_estimators=13, score=0.722, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=2, n_estimators=13, score=0.722, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=2, n_estimators=13, score=0.726, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=4, n_estimators=13, score=0.724, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=4, n_estimators=13, score=0.728, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=4, n_estimators=13, score=0.722, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=4, n_estimators=13, score=0.722, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=4, n_estimators=13, score=0.726, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=6, n_estimators=13, score=0.724, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=6, n_estimators=13, score=0.728, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=6, n_estimators=13, score=0.722, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=6, n_estimators=13, score=0.722, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=6, n_estimators=13, score=0.726, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=8, n_estimators=13, score=0.724, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=8, n_estimators=13, score=0.728, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=8, n_estimators=13, score=0.722, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=8, n_estimators=13, score=0.722, total=   0.5s\n",
      "[CV] criterion=gini, min_samples_leaf=10, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=gini, min_samples_leaf=10, min_samples_split=8, n_estimators=13, score=0.726, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=2, n_estimators=13, score=0.684, total=   0.8s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=2, n_estimators=13, score=0.680, total=   0.8s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=2, n_estimators=13, score=0.667, total=   0.8s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=2, n_estimators=13, score=0.676, total=   0.8s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=2, n_estimators=13, score=0.677, total=   0.8s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=4, n_estimators=13, score=0.685, total=   0.8s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=4, n_estimators=13, score=0.686, total=   0.8s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=4, n_estimators=13, score=0.692, total=   0.8s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=4, n_estimators=13, score=0.700, total=   0.8s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=4, n_estimators=13, score=0.699, total=   0.8s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=6, n_estimators=13, score=0.699, total=   0.7s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=6, n_estimators=13, score=0.703, total=   0.7s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=6, n_estimators=13, score=0.703, total=   0.7s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=6, n_estimators=13, score=0.705, total=   0.8s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=6, n_estimators=13, score=0.702, total=   0.8s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=8, n_estimators=13, score=0.702, total=   0.8s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=8, n_estimators=13, score=0.702, total=   0.7s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=8, n_estimators=13, score=0.706, total=   0.7s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=8, n_estimators=13 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=8, n_estimators=13, score=0.714, total=   0.7s\n",
      "[CV] criterion=entropy, min_samples_leaf=1, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=1, min_samples_split=8, n_estimators=13, score=0.718, total=   0.7s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=2, n_estimators=13, score=0.724, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=2, n_estimators=13, score=0.729, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=2, n_estimators=13, score=0.730, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=2, n_estimators=13, score=0.719, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=2, n_estimators=13, score=0.728, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=4, n_estimators=13, score=0.724, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=4, n_estimators=13, score=0.729, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=4, n_estimators=13, score=0.730, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=4, n_estimators=13, score=0.719, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=4, n_estimators=13, score=0.728, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=6, n_estimators=13, score=0.724, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=6, n_estimators=13, score=0.733, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=6, n_estimators=13, score=0.730, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=6, n_estimators=13, score=0.729, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=6, n_estimators=13, score=0.733, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=8, n_estimators=13, score=0.726, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=8, n_estimators=13, score=0.734, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=8, n_estimators=13, score=0.730, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=8, n_estimators=13, score=0.728, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=2, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=2, min_samples_split=8, n_estimators=13, score=0.732, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=2, n_estimators=13, score=0.733, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=2, n_estimators=13, score=0.734, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=2, n_estimators=13, score=0.721, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=2, n_estimators=13, score=0.731, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=2, n_estimators=13, score=0.731, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=4, n_estimators=13, score=0.733, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=4, n_estimators=13, score=0.734, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=4, n_estimators=13, score=0.721, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=4, n_estimators=13, score=0.731, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=4, n_estimators=13, score=0.731, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=6, n_estimators=13, score=0.733, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=6, n_estimators=13, score=0.734, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=6, n_estimators=13, score=0.721, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=6, n_estimators=13, score=0.731, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=6, n_estimators=13, score=0.731, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=8, n_estimators=13, score=0.725, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=8, n_estimators=13, score=0.733, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=8, n_estimators=13, score=0.727, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=8, n_estimators=13, score=0.728, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=3, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=3, min_samples_split=8, n_estimators=13, score=0.730, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=2, n_estimators=13, score=0.723, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=2, n_estimators=13 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=2, n_estimators=13, score=0.731, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=2, n_estimators=13, score=0.717, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=2, n_estimators=13, score=0.732, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=2, n_estimators=13, score=0.736, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=4, n_estimators=13, score=0.723, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=4, n_estimators=13, score=0.731, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=4, n_estimators=13, score=0.717, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=4, n_estimators=13, score=0.732, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=4, n_estimators=13, score=0.736, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=6, n_estimators=13, score=0.723, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=6, n_estimators=13, score=0.731, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=6, n_estimators=13, score=0.717, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=6, n_estimators=13, score=0.732, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=6, n_estimators=13, score=0.736, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=8, n_estimators=13, score=0.723, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=8, n_estimators=13, score=0.731, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=8, n_estimators=13, score=0.717, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=8, n_estimators=13, score=0.732, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=4, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=4, min_samples_split=8, n_estimators=13, score=0.736, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=2, n_estimators=13, score=0.723, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=2, n_estimators=13, score=0.739, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=2, n_estimators=13, score=0.731, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=2, n_estimators=13, score=0.724, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=2, n_estimators=13, score=0.733, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=4, n_estimators=13, score=0.723, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=4, n_estimators=13, score=0.739, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=4, n_estimators=13, score=0.731, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=4, n_estimators=13, score=0.724, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=4, n_estimators=13, score=0.733, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=6, n_estimators=13, score=0.723, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=6, n_estimators=13, score=0.739, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=6, n_estimators=13, score=0.731, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=6, n_estimators=13, score=0.724, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=6, n_estimators=13, score=0.733, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=8, n_estimators=13, score=0.723, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=8, n_estimators=13, score=0.739, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=8, n_estimators=13, score=0.731, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=8, n_estimators=13, score=0.724, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=5, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=5, min_samples_split=8, n_estimators=13, score=0.733, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=2, n_estimators=13, score=0.730, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=2, n_estimators=13, score=0.741, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=2, n_estimators=13, score=0.724, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=2, n_estimators=13, score=0.729, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=2, n_estimators=13 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=2, n_estimators=13, score=0.722, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=4, n_estimators=13, score=0.730, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=4, n_estimators=13, score=0.741, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=4, n_estimators=13, score=0.724, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=4, n_estimators=13, score=0.729, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=4, n_estimators=13, score=0.722, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=6, n_estimators=13, score=0.730, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=6, n_estimators=13, score=0.741, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=6, n_estimators=13, score=0.724, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=6, n_estimators=13, score=0.729, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=6, n_estimators=13, score=0.722, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=8, n_estimators=13, score=0.730, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=8, n_estimators=13, score=0.741, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=8, n_estimators=13, score=0.724, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=8, n_estimators=13, score=0.729, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=6, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=6, min_samples_split=8, n_estimators=13, score=0.722, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=2, n_estimators=13, score=0.719, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=2, n_estimators=13, score=0.738, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=2, n_estimators=13, score=0.725, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=2, n_estimators=13, score=0.726, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=2, n_estimators=13, score=0.741, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=4, n_estimators=13, score=0.719, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=4, n_estimators=13, score=0.738, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=4, n_estimators=13, score=0.725, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=4, n_estimators=13, score=0.726, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=4, n_estimators=13, score=0.741, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=6, n_estimators=13, score=0.719, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=6, n_estimators=13, score=0.738, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=6, n_estimators=13, score=0.725, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=6, n_estimators=13, score=0.726, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=6, n_estimators=13, score=0.741, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=8, n_estimators=13, score=0.719, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=8, n_estimators=13, score=0.738, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=8, n_estimators=13, score=0.725, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=8, n_estimators=13, score=0.726, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=7, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=7, min_samples_split=8, n_estimators=13, score=0.741, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=2, n_estimators=13, score=0.720, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=2, n_estimators=13, score=0.738, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=2, n_estimators=13, score=0.722, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=2, n_estimators=13, score=0.724, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=2, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=2, n_estimators=13, score=0.737, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=4, n_estimators=13, score=0.720, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=4, n_estimators=13, score=0.738, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=4, n_estimators=13 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=4, n_estimators=13, score=0.722, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=4, n_estimators=13, score=0.724, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=4, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=4, n_estimators=13, score=0.737, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=6, n_estimators=13, score=0.720, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=6, n_estimators=13, score=0.738, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=6, n_estimators=13, score=0.722, total=   0.6s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=6, n_estimators=13, score=0.724, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=6, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=6, n_estimators=13, score=0.737, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=8, n_estimators=13, score=0.720, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=8, n_estimators=13, score=0.738, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=8, n_estimators=13, score=0.722, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=8, n_estimators=13, score=0.724, total=   0.5s\n",
      "[CV] criterion=entropy, min_samples_leaf=10, min_samples_split=8, n_estimators=13 \n",
      "[CV]  criterion=entropy, min_samples_leaf=10, min_samples_split=8, n_estimators=13, score=0.737, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:  3.1min finished\n",
      "Unoptimized model\n",
      "------\n",
      "Accuracy score on testing data: 0.8420\n",
      "F-score on testing data: 0.6802\n",
      "\n",
      "Optimized Model\n",
      "------\n",
      "Final accuracy score on the testing data: 0.8590\n",
      "Final F-score on the testing data: 0.7285\n"
     ]
    }
   ],
   "source": [
    "# Importando 'GridSearchCV', 'make_scorer' e RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Inicializando o classificador\n",
    "clf = RandomForestClassifier(random_state = 10)\n",
    "\n",
    "# Criando a lista de parâmetros para otimizar.\n",
    "parameters =  {\n",
    "                \"min_samples_split\" : [2,4,6,8],\n",
    "                \"min_samples_leaf\" : [1, 2, 3, 4, 5, 6, 7, 10],\n",
    "                \"criterion\": [\"gini\",\"entropy\"],\n",
    "                \"n_estimators\": [13]\n",
    "              }\n",
    "\n",
    "# Criando um objeto fbeta_score utilizando make_scorer()\n",
    "scorer = make_scorer(fbeta_score, beta=0.5)\n",
    "\n",
    "# Realizando uma busca grid no classificador utilizando o 'scorer' como o método de score no GridSearchCV() \n",
    "grid_obj = GridSearchCV(clf, parameters, scoring = scorer, verbose=100)\n",
    "\n",
    "# Adequando o objeto da busca grid como os dados para treinamento e encontrar os parâmetros ótimos utilizando fit() \n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Recuperando o estimador\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Realizando predições utilizando o modelo não otimizado e modelar\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_test)\n",
    "best_predictions = best_clf.predict(X_test)\n",
    "\n",
    "# Reportando os scores de antes e de depois\n",
    "print (\"Unoptimized model\\n------\")\n",
    "print (\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
    "print (\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5)))\n",
    "print (\"\\nOptimized Model\\n------\")\n",
    "print (\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n",
    "print (\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação final do modelo\n",
    "\n",
    "Antes da implementação do modelo, se escolhessemos um registro ao acaso, teríamos uma probabilidade de 24,78% de obter um doador. Após a análise utilizando Florestas Aleatórias este valor passa uma Accuracy de 85,64% e F-score 66,96, provando-se bastante superior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     Metric     | Unoptimized Model | Optimized Model |\n",
    "| :------------: | :---------------: | :-------------: | \n",
    "| Accuracy Score |    84.20%         |   85,90%        |\n",
    "| F-score        |    68.02%         |   72,85%        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Importância dos atributos\n",
    "\n",
    "Uma tarefa importante quando realizamos aprendizado supervisionado em um conjunto de dados como os dados do censo que estudamos aqui é determinar quais atributos fornecem maior poder de predição. Focando no relacionamento entre alguns poucos atributos mais importantes e na label alvo nós simplificamos muito o nosso entendimento do fenômeno, que é a coisa mais importante a se fazer. No caso deste projeto, isso significa que nós queremos identificar um pequeno número de atributos que possuem maior chance de predizer se um indivíduo possui renda anual superior à \\$50,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação - Extraindo a importância do atributo\n",
    "Escolha um algoritmo de aprendizado supervisionado da `sciki-learn` que possui o atributo `feature_importance_` disponível. Agora utilizaremos o atributo `feature_importance_` que é uma função que ranqueia a importância de cada atributo dos registros do conjunto de dados quando realizamos predições baseadas no algoritmo escolhido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Treinando o modelo utilizando o conjunto de treinamento com .fit(X_train, y_train)\n",
    "model = clf.fit(X_train, y_train)\n",
    "\n",
    "# Extraindo a importância dos atributos utilizando .feature_importances_ \n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Plotar no gráfico\n",
    "vs.feature_plot(importances, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observação da Relevância dos Atributos\n",
    "Quando **Exploramos os dados**, vimos que existem treze atributos disponíveis para cada registro nos dados do censo. Dos 13 atributos os 5 abaixo são os mais relevantes:\n",
    "1. Capital-gain: quanto maior o salário, maior a probabilidae de doações;\n",
    "2. Marital Status: pessoas que constroem uma família tem uma maior predisposição a doar;\n",
    "3. Education: Quanto maior o grau de educação, mais se entende a necessidade de ajudar ao próximo;\n",
    "4. Age: pessoas mais velhas tendem a ajudar mais o próximo;\n",
    "5. Relationship: demonstrando que pessoas casadas são mais propensas à doação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecionando atributos\n",
    "\n",
    "A partir da visualização acima, nós vemos que os cinco atributos mais importantes contribuem para ~60% da importância de **todos** os atributos presentes nos dados. Isto indica que nós podemos tentar *reduzir os atributos* e simplificar a informação necessária para o modelo aprender em um tempo muito menor. O código abaixo utilizará o mesmo modelo otimizado que encontramos anteriormente e treinará o modelo com o mesmo conjunto de dados de treinamento, porém apenas com *os cinco atributos mais importantes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model trained on full data\n",
      "------\n",
      "Accuracy on testing data: 0.8590\n",
      "F-score on testing data: 0.7285\n",
      "\n",
      "Final Model trained on reduced data\n",
      "------\n",
      "Accuracy on testing data: 0.8446\n",
      "F-score on testing data: 0.6913\n"
     ]
    }
   ],
   "source": [
    "# Importar a funcionalidade para clonar um modelo\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Reduzir a quantidade de atributos\n",
    "X_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::-1])[:5]]]\n",
    "X_test_reduced = X_test[X_test.columns.values[(np.argsort(importances)[::-1])[:5]]]\n",
    "\n",
    "# Treinar o melhor modelo encontrado com a busca grid anterior\n",
    "clf = (clone(best_clf)).fit(X_train_reduced, y_train)\n",
    "\n",
    "# Fazer novas predições\n",
    "reduced_predictions = clf.predict(X_test_reduced)\n",
    "\n",
    "# Reportar os scores do modelo final utilizando as duas versões dos dados.\n",
    "print (\"Final Model trained on full data\\n------\")\n",
    "print (\"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n",
    "print (\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))\n",
    "print (\"\\nFinal Model trained on reduced data\\n------\")\n",
    "print (\"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, reduced_predictions)))\n",
    "print (\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, reduced_predictions, beta = 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 24.0, 'Predicted')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns # Install using 'pip install seaborn'\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "cm_test = confusion_matrix(y_test, best_clf.predict(X_test))\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.heatmap(cm_test, annot=True, cmap='Greys', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.title('Confusion Matrix for the Test Set')\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questão 8 - Efeitos da seleção de atributos\n",
    "\n",
    "A accuracy e o F-score tiveram um ligeiro recuo ao utilizar a base apenas com as principais features. Portanto, a base enxuta só deveria ser utilizada se fosse necessário treinar os dados numa frequência alta. Como no nosso problema este conjunto de dados não seria treinado com muita frequência, é recomendado que utilize a base completa para uma maior precisão.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
